# FrawdLLM

A 109M parameter language model trained from scratch, implementing the full LLM pipeline: Pre-training → SFT → DPO.

**Model on HuggingFace**: [tsingla98/frawdllm-100m](https://huggingface.co/tsingla98/frawdllm-100m)

## Overview

This project is a hands-on learning exercise to understand every stage of LLM development:

- **Pre-training**: Next-token prediction on OpenWebText (~2B tokens)
- **SFT**: Instruction fine-tuning on 39K examples
- **DPO**: Preference alignment using 15K ranked pairs

### Model Architecture

| Parameter | Value |
|-----------|-------|
| Parameters | 109M |
| Layers | 12 |
| Hidden Size | 768 |
| Attention Heads | 12 |
| Context Length | 1024 |
| Vocab Size | 32,000 |
| Position Encoding | RoPE |

## Quick Start

### Load from HuggingFace

```python
from transformers import AutoModelForCausalLM, PreTrainedTokenizerFast
import torch

model = AutoModelForCausalLM.from_pretrained(
    "tsingla98/frawdllm-100m",
    trust_remote_code=True
)
tokenizer = PreTrainedTokenizerFast.from_pretrained("tsingla98/frawdllm-100m")

prompt = "<|bos|><|user|>What is photosynthesis?<|assistant|>"
inputs = tokenizer(prompt, return_tensors="pt", add_special_tokens=False)

with torch.no_grad():
    output = model.model.generate(inputs.input_ids, max_new_tokens=100, temperature=0.8, top_k=50)

print(tokenizer.decode(output[0]))
```

### Train from Scratch

```bash
# Setup
uv sync

# Pre-training on Modal
modal run scripts/train_modal_100m.py

# SFT
modal run scripts/sft_modal_100m.py

# DPO
modal run scripts/dpo_modal_100m.py
```

## Project Structure

```
FrawdLLM/
├── src/
│   ├── model/           # GPT architecture (attention, MLP, embeddings, RoPE)
│   ├── training/        # Trainer, datasets
│   └── fetch_data/      # Data generation, tokenization
├── scripts/
│   ├── train_modal_100m.py    # Pre-training on Modal
│   ├── sft_modal_100m.py      # SFT training
│   ├── dpo_modal_100m.py      # DPO training
│   ├── generate_dpo_modal.py  # Generate DPO response pairs
│   └── chat.py                # Interactive chat
└── data/
    ├── sft_100m/        # SFT instruction data
    └── dpo_100m/        # DPO preference data
```

## Training Pipeline

### 1. Pre-training
- OpenWebText dataset (~8M documents)
- BPE tokenizer with 32K vocab
- Trained for ~2B tokens on Modal A100

### 2. SFT (Supervised Fine-tuning)
- 39K instruction-response pairs
- Generated using Claude across 8 categories
- 3 epochs, final val_loss: 2.38

### 3. DPO (Direct Preference Optimization)
- 15K preference pairs (chosen/rejected)
- Responses generated by SFT model, ranked by Claude
- 3 epochs, final val_accuracy: 63%

## Run Locally

```python
import torch
from src.model.gpt import FrawdLLM
from src.fetch_data.tokenizer import load_tokenizer
from pathlib import Path

# Load checkpoint
checkpoint = torch.load("checkpoints_100m_dpo/best.pt", map_location="cpu", weights_only=False)
model = FrawdLLM(checkpoint["config"])
model.load_state_dict(checkpoint["model_state_dict"])
model.eval()

# Load tokenizer
tokenizer = load_tokenizer(Path("tokenizer_100m/tokenizer.json"))
tokenizer.add_special_tokens(["<|user|>", "<|assistant|>"])

# Generate
prompt = "<|bos|><|user|>What is the sun?<|assistant|>"
input_ids = torch.tensor([tokenizer.encode(prompt, add_special_tokens=False).ids])

with torch.no_grad():
    output = model.generate(input_ids, max_new_tokens=100, temperature=0.8, top_k=50)

print(tokenizer.decode(output[0].tolist()))
```

## License

MIT
