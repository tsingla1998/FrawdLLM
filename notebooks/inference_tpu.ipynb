{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrawdLLM Inference on TPU (JAX)\n",
    "\n",
    "This notebook implements transformer inference from scratch using JAX, running on Google TPU.\n",
    "\n",
    "**Setup:** Runtime > Change runtime type > TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install huggingface_hub safetensors tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify TPU is available\n",
    "import jax\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "print(f\"Device count: {jax.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Weights from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Download weights and tokenizer from HuggingFace\n",
    "weights_path = hf_hub_download(repo_id=\"tsingla1998/frawdllm-100m\", filename=\"model.safetensors\")\n",
    "tokenizer_path = hf_hub_download(repo_id=\"tsingla1998/frawdllm-100m\", filename=\"tokenizer.json\")\n",
    "\n",
    "print(f\"Weights: {weights_path}\")\n",
    "print(f\"Tokenizer: {tokenizer_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Load weights into JAX arrays\n",
    "weights = {}\n",
    "with safe_open(weights_path, framework=\"numpy\") as f:\n",
    "    for key in f.keys():\n",
    "        weights[key] = jnp.array(f.get_tensor(key))\n",
    "        print(f\"{key}: {weights[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "N_HEADS = 12\n",
    "N_LAYERS = 12\n",
    "HEAD_DIM = 64\n",
    "N_EMBD = N_HEADS * HEAD_DIM  # 768\n",
    "\n",
    "# Generation\n",
    "STOP_TOKEN_ID = 3\n",
    "TEMPERATURE = 0.5\n",
    "TOP_P = 0.9\n",
    "TOP_K = 100\n",
    "MAX_OUTPUT_TOKENS = 300\n",
    "ROPE_THETA = 10000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Implementation\n",
    "\n",
    "TODO: Implement the following in JAX:\n",
    "- RoPE (Rotary Position Embeddings)\n",
    "- LayerNorm\n",
    "- Attention\n",
    "- MLP\n",
    "- Generation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
