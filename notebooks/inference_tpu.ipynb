{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrawdLLM Inference on TPU (JAX)\n",
    "\n",
    "This notebook implements transformer inference from scratch using JAX, running on Google TPU.\n",
    "\n",
    "**Setup:** Runtime > Change runtime type > TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install huggingface_hub safetensors tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify TPU is available\n",
    "import jax\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "print(f\"Device count: {jax.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Weights from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Download weights and tokenizer from HuggingFace\n",
    "weights_path = hf_hub_download(repo_id=\"tsingla98/frawdllm-100m\", filename=\"model.safetensors\")\n",
    "tokenizer_path = hf_hub_download(repo_id=\"tsingla98/frawdllm-100m\", filename=\"tokenizer.json\")\n",
    "\n",
    "print(f\"Weights: {weights_path}\")\n",
    "print(f\"Tokenizer: {tokenizer_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Load weights into JAX arrays\n",
    "weights = {}\n",
    "with safe_open(weights_path, framework=\"numpy\") as f:\n",
    "    for key in f.keys():\n",
    "        weights[key] = jnp.array(f.get_tensor(key))\n",
    "        print(f\"{key}: {weights[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import math\n",
    "\n",
    "EMBEDDINGS_WEIGHT_KEY = \"model.embeddings.token_emb.weight\"\n",
    "\n",
    "LN1_WEIGHT_FORMAT_PRE_ATTN_KEY = \"model.blocks.{}.ln1.weight\"\n",
    "LN1_BIAS_FORMAT_PRE_ATTN_KEY = \"model.blocks.{}.ln1.bias\"\n",
    "\n",
    "LN2_WEIGHT_FORMAT_PRE_ATTN_KEY = \"model.blocks.{}.ln2.weight\"\n",
    "LN2_BIAS_FORMAT_PRE_ATTN_KEY = \"model.blocks.{}.ln2.bias\"\n",
    "\n",
    "LNF_WEIGHT_FORMAT_KEY = \"model.ln_f.weight\"\n",
    "LNF_BIAS_FORMAT_KEY = \"model.ln_f.bias\"\n",
    "\n",
    "LM_HEAD_WEIGHT_KEY = \"model.embeddings.token_emb.weight\"  # tied weights\n",
    "\n",
    "QKV_WEIGHTS_KEY = \"model.blocks.{}.attn.qkv_proj.weight\"\n",
    "QKV_BIAS_KEY = \"model.blocks.{}.attn.qkv_proj.bias\"\n",
    "\n",
    "OUTPUT_PROJ_BIAS_KEY = \"model.blocks.{}.attn.out_proj.bias\"\n",
    "OUTPUT_PROJ_WEIGHT_KEY = \"model.blocks.{}.attn.out_proj.weight\"\n",
    "\n",
    "FC1_WEIGHT_KEY = \"model.blocks.{}.mlp.fc1.weight\"\n",
    "FC1_BIAS_KEY = \"model.blocks.{}.mlp.fc1.bias\"\n",
    "\n",
    "FC2_WEIGHT_KEY = \"model.blocks.{}.mlp.fc2.weight\"\n",
    "FC2_BIAS_KEY = \"model.blocks.{}.mlp.fc2.bias\"\n",
    "\n",
    "STOP_TOKEN_ID = 3\n",
    "\n",
    "EPSILON = 1e-5\n",
    "N_HEADS = 12\n",
    "N_LAYERS = 12\n",
    "HEAD_DIM = 64\n",
    "EMBEDDINGS_DIM = N_HEADS * HEAD_DIM\n",
    "ROPE_THETA = 10000.0\n",
    "TEMPERATURE = 0.5\n",
    "TOP_P = 0.9\n",
    "\n",
    "MAX_OUTPUT_TOKENS = 300\n",
    "MAX_CONTEXT_LENGTH = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Implementation (JAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def format_prompt(user_message: str) -> str:\n",
    "    return f\"<|bos|><|user|>{user_message}<|assistant|>\"\n",
    "\n",
    "\n",
    "def get_weights_tensor(key: str) -> jnp.ndarray:\n",
    "    return weights[key]\n",
    "\n",
    "\n",
    "def get_tokens_for_prompt(prompt: str) -> jnp.ndarray:\n",
    "    return jnp.array(tokenizer.encode(prompt, add_special_tokens=False).ids)\n",
    "\n",
    "\n",
    "# Precompute RoPE frequencies for max context length\n",
    "def _compute_rope_freqs(max_len: int) -> jnp.ndarray:\n",
    "    dim_indices = jnp.arange(0, HEAD_DIM, 2).astype(jnp.float32)\n",
    "    freqs = 1.0 / (ROPE_THETA ** (dim_indices / HEAD_DIM))\n",
    "    positions = jnp.arange(max_len).astype(jnp.float32)\n",
    "    angles = jnp.outer(positions, freqs)\n",
    "    return angles\n",
    "\n",
    "ROPE_FREQS = _compute_rope_freqs(MAX_CONTEXT_LENGTH)\n",
    "\n",
    "# Fixed prompt padding size - compile once, run with any prompt up to this size\n",
    "PROMPT_PAD_SIZE = 256\n",
    "\n",
    "\n",
    "# === Stack all layer weights for vectorized access ===\n",
    "def _stack_weights():\n",
    "    \"\"\"Stack weights across layers for efficient access.\"\"\"\n",
    "    ln1_weights = jnp.stack([get_weights_tensor(LN1_WEIGHT_FORMAT_PRE_ATTN_KEY.format(i)) for i in range(N_LAYERS)])\n",
    "    ln1_biases = jnp.stack([get_weights_tensor(LN1_BIAS_FORMAT_PRE_ATTN_KEY.format(i)) for i in range(N_LAYERS)])\n",
    "    ln2_weights = jnp.stack([get_weights_tensor(LN2_WEIGHT_FORMAT_PRE_ATTN_KEY.format(i)) for i in range(N_LAYERS)])\n",
    "    ln2_biases = jnp.stack([get_weights_tensor(LN2_BIAS_FORMAT_PRE_ATTN_KEY.format(i)) for i in range(N_LAYERS)])\n",
    "    qkv_weights = jnp.stack([get_weights_tensor(QKV_WEIGHTS_KEY.format(i)) for i in range(N_LAYERS)])\n",
    "    qkv_biases = jnp.stack([get_weights_tensor(QKV_BIAS_KEY.format(i)) for i in range(N_LAYERS)])\n",
    "    out_weights = jnp.stack([get_weights_tensor(OUTPUT_PROJ_WEIGHT_KEY.format(i)) for i in range(N_LAYERS)])\n",
    "    out_biases = jnp.stack([get_weights_tensor(OUTPUT_PROJ_BIAS_KEY.format(i)) for i in range(N_LAYERS)])\n",
    "    fc1_weights = jnp.stack([get_weights_tensor(FC1_WEIGHT_KEY.format(i)) for i in range(N_LAYERS)])\n",
    "    fc1_biases = jnp.stack([get_weights_tensor(FC1_BIAS_KEY.format(i)) for i in range(N_LAYERS)])\n",
    "    fc2_weights = jnp.stack([get_weights_tensor(FC2_WEIGHT_KEY.format(i)) for i in range(N_LAYERS)])\n",
    "    fc2_biases = jnp.stack([get_weights_tensor(FC2_BIAS_KEY.format(i)) for i in range(N_LAYERS)])\n",
    "    \n",
    "    return {\n",
    "        'ln1_w': ln1_weights, 'ln1_b': ln1_biases,\n",
    "        'ln2_w': ln2_weights, 'ln2_b': ln2_biases,\n",
    "        'qkv_w': qkv_weights, 'qkv_b': qkv_biases,\n",
    "        'out_w': out_weights, 'out_b': out_biases,\n",
    "        'fc1_w': fc1_weights, 'fc1_b': fc1_biases,\n",
    "        'fc2_w': fc2_weights, 'fc2_b': fc2_biases,\n",
    "        'ln_f_w': get_weights_tensor(LNF_WEIGHT_FORMAT_KEY),\n",
    "        'ln_f_b': get_weights_tensor(LNF_BIAS_FORMAT_KEY),\n",
    "        'embed': get_weights_tensor(EMBEDDINGS_WEIGHT_KEY),\n",
    "        'lm_head': get_weights_tensor(LM_HEAD_WEIGHT_KEY),\n",
    "    }\n",
    "\n",
    "STACKED_WEIGHTS = _stack_weights()\n",
    "\n",
    "\n",
    "def apply_rope(x: jnp.ndarray, angles: jnp.ndarray) -> jnp.ndarray:\n",
    "    x_even = x[..., 0::2]\n",
    "    x_odd = x[..., 1::2]\n",
    "    cos = jnp.cos(angles)\n",
    "    sin = jnp.sin(angles)\n",
    "    x_even_rot = x_even * cos - x_odd * sin\n",
    "    x_odd_rot = x_even * sin + x_odd * cos\n",
    "    out = jnp.stack([x_even_rot, x_odd_rot], axis=-1)\n",
    "    out = out.reshape(x.shape)\n",
    "    return out\n",
    "\n",
    "\n",
    "def layer_norm(x, gamma, beta):\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    std = x.std(axis=-1, keepdims=True)\n",
    "    return (x - mean) / (std + EPSILON) * gamma + beta\n",
    "\n",
    "\n",
    "def decode_layer(layer_idx, x, k_cache, v_cache, cache_len, W):\n",
    "    \"\"\"Process one layer during decode (single token).\"\"\"\n",
    "    normed = layer_norm(x, W['ln1_w'][layer_idx], W['ln1_b'][layer_idx])\n",
    "    \n",
    "    qkv = normed @ W['qkv_w'][layer_idx].T + W['qkv_b'][layer_idx]\n",
    "    q, k, v = jnp.split(qkv, 3, axis=-1)\n",
    "    \n",
    "    q = q.reshape(1, N_HEADS, HEAD_DIM).swapaxes(0, 1)\n",
    "    k = k.reshape(1, N_HEADS, HEAD_DIM).swapaxes(0, 1)\n",
    "    v = v.reshape(1, N_HEADS, HEAD_DIM).swapaxes(0, 1)\n",
    "    \n",
    "    angles = jax.lax.dynamic_slice(ROPE_FREQS, (cache_len, 0), (1, HEAD_DIM // 2))\n",
    "    q = apply_rope(q, angles)\n",
    "    k = apply_rope(k, angles)\n",
    "    \n",
    "    k_cache = k_cache.at[layer_idx, cache_len, :, :].set(k.swapaxes(0, 1)[0])\n",
    "    v_cache = v_cache.at[layer_idx, cache_len, :, :].set(v.swapaxes(0, 1)[0])\n",
    "    \n",
    "    k_full = k_cache[layer_idx].swapaxes(0, 1)\n",
    "    v_full = v_cache[layer_idx].swapaxes(0, 1)\n",
    "    \n",
    "    scores = (q @ k_full.swapaxes(-2, -1)) / math.sqrt(HEAD_DIM)\n",
    "    positions = jnp.arange(MAX_CONTEXT_LENGTH)\n",
    "    mask = jnp.where(positions <= cache_len, 0.0, -1e9)\n",
    "    scores = scores + mask\n",
    "    \n",
    "    attn = jax.nn.softmax(scores, axis=-1)\n",
    "    out = (attn @ v_full).swapaxes(0, 1).reshape(1, EMBEDDINGS_DIM)\n",
    "    \n",
    "    out = out @ W['out_w'][layer_idx].T + W['out_b'][layer_idx] + x\n",
    "    \n",
    "    normed2 = layer_norm(out, W['ln2_w'][layer_idx], W['ln2_b'][layer_idx])\n",
    "    hidden = jax.nn.gelu(normed2 @ W['fc1_w'][layer_idx].T + W['fc1_b'][layer_idx])\n",
    "    out = hidden @ W['fc2_w'][layer_idx].T + W['fc2_b'][layer_idx] + out\n",
    "    \n",
    "    return out, k_cache, v_cache\n",
    "\n",
    "\n",
    "def prefill_layer_padded(layer_idx, x, k_cache, v_cache, actual_len, W):\n",
    "    \"\"\"Process one layer during prefill with padding support.\"\"\"\n",
    "    # x: [PROMPT_PAD_SIZE, 768], actual_len is traced\n",
    "    pad_size = PROMPT_PAD_SIZE\n",
    "    \n",
    "    normed = layer_norm(x, W['ln1_w'][layer_idx], W['ln1_b'][layer_idx])\n",
    "    \n",
    "    qkv = normed @ W['qkv_w'][layer_idx].T + W['qkv_b'][layer_idx]\n",
    "    q, k, v = jnp.split(qkv, 3, axis=-1)\n",
    "    \n",
    "    q = q.reshape(pad_size, N_HEADS, HEAD_DIM).swapaxes(0, 1)\n",
    "    k = k.reshape(pad_size, N_HEADS, HEAD_DIM).swapaxes(0, 1)\n",
    "    v = v.reshape(pad_size, N_HEADS, HEAD_DIM).swapaxes(0, 1)\n",
    "    \n",
    "    # RoPE - use full padded size\n",
    "    angles = ROPE_FREQS[:pad_size]\n",
    "    q = apply_rope(q, angles)\n",
    "    k = apply_rope(k, angles)\n",
    "    \n",
    "    # Store in cache (only actual_len matters, but store all for simplicity)\n",
    "    k_cache = k_cache.at[layer_idx, :pad_size, :, :].set(k.swapaxes(0, 1))\n",
    "    v_cache = v_cache.at[layer_idx, :pad_size, :, :].set(v.swapaxes(0, 1))\n",
    "    \n",
    "    # Attention with combined causal + padding mask\n",
    "    scores = (q @ k.swapaxes(-2, -1)) / math.sqrt(HEAD_DIM)\n",
    "    \n",
    "    # Causal mask: position i can only attend to j where j <= i\n",
    "    row_idx = jnp.arange(pad_size)[:, None]  # [pad_size, 1]\n",
    "    col_idx = jnp.arange(pad_size)[None, :]  # [1, pad_size]\n",
    "    causal_mask = jnp.where(col_idx <= row_idx, 0.0, -1e9)  # [pad_size, pad_size]\n",
    "    \n",
    "    # Padding mask: can only attend to positions < actual_len\n",
    "    padding_mask = jnp.where(col_idx < actual_len, 0.0, -1e9)  # [1, pad_size]\n",
    "    \n",
    "    # Combined mask\n",
    "    combined_mask = causal_mask + padding_mask  # broadcasts to [pad_size, pad_size]\n",
    "    scores = scores + combined_mask\n",
    "    \n",
    "    attn = jax.nn.softmax(scores, axis=-1)\n",
    "    out = (attn @ v).swapaxes(0, 1).reshape(pad_size, EMBEDDINGS_DIM)\n",
    "    \n",
    "    out = out @ W['out_w'][layer_idx].T + W['out_b'][layer_idx] + x\n",
    "    \n",
    "    normed2 = layer_norm(out, W['ln2_w'][layer_idx], W['ln2_b'][layer_idx])\n",
    "    hidden = jax.nn.gelu(normed2 @ W['fc1_w'][layer_idx].T + W['fc1_b'][layer_idx])\n",
    "    out = hidden @ W['fc2_w'][layer_idx].T + W['fc2_b'][layer_idx] + out\n",
    "    \n",
    "    return out, k_cache, v_cache\n",
    "\n",
    "\n",
    "def sample_token(logits, key):\n",
    "    \"\"\"Sample next token with temperature and top-p.\"\"\"\n",
    "    probs = jax.nn.softmax(logits / TEMPERATURE)\n",
    "    top_probs, top_indices = jax.lax.top_k(probs, k=100)\n",
    "    sorted_idx = jnp.argsort(top_probs)[::-1]\n",
    "    sorted_probs = top_probs[sorted_idx]\n",
    "    cum_probs = jnp.cumsum(sorted_probs)\n",
    "    mask = (cum_probs <= TOP_P).at[0].set(True)\n",
    "    sorted_probs = jnp.where(mask, sorted_probs, 0.0)\n",
    "    \n",
    "    key, subkey = jax.random.split(key)\n",
    "    sampled = jax.random.categorical(subkey, jnp.log(sorted_probs + 1e-10))\n",
    "    next_token = top_indices[sorted_idx[sampled]]\n",
    "    \n",
    "    return next_token, key\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnums=(2,))\n",
    "def generate(padded_tokens, actual_len, max_new_tokens, W, key):\n",
    "    \"\"\"\n",
    "    Full generation loop in XLA - no Python sync points.\n",
    "    \n",
    "    Args:\n",
    "        padded_tokens: [PROMPT_PAD_SIZE] - prompt tokens padded with zeros\n",
    "        actual_len: traced int - actual prompt length\n",
    "        max_new_tokens: static int - max tokens to generate\n",
    "        W: weights dict\n",
    "        key: PRNG key\n",
    "    \n",
    "    Returns: (output_tokens, num_generated)\n",
    "    \"\"\"\n",
    "    # Initialize KV cache\n",
    "    k_cache = jnp.zeros((N_LAYERS, MAX_CONTEXT_LENGTH, N_HEADS, HEAD_DIM))\n",
    "    v_cache = jnp.zeros((N_LAYERS, MAX_CONTEXT_LENGTH, N_HEADS, HEAD_DIM))\n",
    "    \n",
    "    # === PREFILL (padded) ===\n",
    "    x = W['embed'][padded_tokens]  # [PROMPT_PAD_SIZE, 768]\n",
    "    \n",
    "    for layer_idx in range(N_LAYERS):\n",
    "        x, k_cache, v_cache = prefill_layer_padded(layer_idx, x, k_cache, v_cache, actual_len, W)\n",
    "    \n",
    "    x = layer_norm(x, W['ln_f_w'], W['ln_f_b'])\n",
    "    \n",
    "    # Get logits for last REAL token (not last padded position)\n",
    "    last_hidden = jax.lax.dynamic_slice(x, (actual_len - 1, 0), (1, EMBEDDINGS_DIM))[0]\n",
    "    logits = last_hidden @ W['lm_head'].T\n",
    "    \n",
    "    # Sample first token\n",
    "    first_token, key = sample_token(logits, key)\n",
    "    \n",
    "    # Initialize output buffer\n",
    "    output_tokens = jnp.zeros(max_new_tokens, dtype=jnp.int32)\n",
    "    output_tokens = output_tokens.at[0].set(first_token)\n",
    "    \n",
    "    # === DECODE with while_loop ===\n",
    "    init_state = (\n",
    "        output_tokens,\n",
    "        jnp.array(1, dtype=jnp.int32),  # num_generated\n",
    "        k_cache,\n",
    "        v_cache,\n",
    "        actual_len,  # cache_len starts at actual prompt length\n",
    "        key,\n",
    "        first_token == STOP_TOKEN_ID,  # done\n",
    "    )\n",
    "    \n",
    "    def cond_fn(state):\n",
    "        _, num_generated, _, _, _, _, done = state\n",
    "        return jnp.logical_and(num_generated < max_new_tokens, ~done)\n",
    "    \n",
    "    def body_fn(state):\n",
    "        output_tokens, num_generated, k_cache, v_cache, cache_len, key, _ = state\n",
    "        \n",
    "        last_token = output_tokens[num_generated - 1]\n",
    "        x = W['embed'][last_token].reshape(1, -1)\n",
    "        \n",
    "        for layer_idx in range(N_LAYERS):\n",
    "            x, k_cache, v_cache = decode_layer(layer_idx, x, k_cache, v_cache, cache_len, W)\n",
    "        \n",
    "        x = layer_norm(x, W['ln_f_w'], W['ln_f_b'])\n",
    "        logits = x[0] @ W['lm_head'].T\n",
    "        \n",
    "        next_token, key = sample_token(logits, key)\n",
    "        \n",
    "        output_tokens = output_tokens.at[num_generated].set(next_token)\n",
    "        done = next_token == STOP_TOKEN_ID\n",
    "        \n",
    "        return (output_tokens, num_generated + 1, k_cache, v_cache, cache_len + 1, key, done)\n",
    "    \n",
    "    final_state = jax.lax.while_loop(cond_fn, body_fn, init_state)\n",
    "    \n",
    "    output_tokens, num_generated, _, _, _, _, _ = final_state\n",
    "    return output_tokens, num_generated\n",
    "\n",
    "\n",
    "def pad_tokens(tokens, pad_size=PROMPT_PAD_SIZE):\n",
    "    \"\"\"Pad tokens to fixed size for JIT cache reuse.\"\"\"\n",
    "    padded = jnp.zeros(pad_size, dtype=jnp.int32)\n",
    "    padded = padded.at[:len(tokens)].set(tokens)\n",
    "    return padded, len(tokens)\n",
    "\n",
    "\n",
    "def run_inference(prompt: str, key):\n",
    "    \"\"\"Helper to run inference on a prompt.\"\"\"\n",
    "    formatted = format_prompt(prompt)\n",
    "    tokens = get_tokens_for_prompt(formatted)\n",
    "    \n",
    "    if len(tokens) > PROMPT_PAD_SIZE:\n",
    "        raise ValueError(f\"Prompt too long: {len(tokens)} > {PROMPT_PAD_SIZE}\")\n",
    "    \n",
    "    padded, actual_len = pad_tokens(tokens)\n",
    "    actual_len = jnp.array(actual_len, dtype=jnp.int32)\n",
    "    \n",
    "    output_tokens, num_generated = generate(padded, actual_len, MAX_OUTPUT_TOKENS, STACKED_WEIGHTS, key)\n",
    "    \n",
    "    num_gen = int(num_generated)\n",
    "    out_list = [int(output_tokens[i]) for i in range(num_gen) if int(output_tokens[i]) != STOP_TOKEN_ID]\n",
    "    \n",
    "    return tokenizer.decode(out_list), len(out_list)\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    import time\n",
    "    \n",
    "    print(f\"Prompt pad size: {PROMPT_PAD_SIZE}\")\n",
    "    print(f\"Max new tokens: {MAX_OUTPUT_TOKENS}\")\n",
    "    print()\n",
    "    \n",
    "    # First run - triggers compilation\n",
    "    print(\"=== First prompt (triggers compilation) ===\")\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    \n",
    "    start = time.time()\n",
    "    output, num_tokens = run_inference(\"Hello, World!\", key)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"Time: {elapsed:.3f}s (includes compilation)\")\n",
    "    print(f\"Generated: {num_tokens} tokens\")\n",
    "    print(f\"Output: {output}\")\n",
    "    print()\n",
    "    \n",
    "    # Second run - same compiled code, different prompt\n",
    "    print(\"=== Second prompt (uses cached compilation) ===\")\n",
    "    key = jax.random.PRNGKey(123)\n",
    "    \n",
    "    start = time.time()\n",
    "    output, num_tokens = run_inference(\"What is the capital of France?\", key)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"Time: {elapsed:.3f}s\")\n",
    "    print(f\"Speed: {num_tokens/elapsed:.1f} tok/s\")\n",
    "    print(f\"Output: {output}\")\n",
    "    print()\n",
    "    \n",
    "    # Third run - another prompt\n",
    "    print(\"=== Third prompt (cached) ===\")\n",
    "    key = jax.random.PRNGKey(456)\n",
    "    \n",
    "    start = time.time()\n",
    "    output, num_tokens = run_inference(\"Explain quantum computing in simple terms.\", key)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"Time: {elapsed:.3f}s\")\n",
    "    print(f\"Speed: {num_tokens/elapsed:.1f} tok/s\")\n",
    "    print(f\"Output: {output}\")\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frawdllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
