{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrawdLLM Inference on TPU (JAX)\n",
    "\n",
    "This notebook implements transformer inference from scratch using JAX, running on Google TPU.\n",
    "\n",
    "**Setup:** Runtime > Change runtime type > TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install huggingface_hub safetensors tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify TPU is available\n",
    "import jax\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "print(f\"Device count: {jax.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Weights from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Download weights and tokenizer from HuggingFace\n",
    "weights_path = hf_hub_download(repo_id=\"tsingla98/frawdllm-100m\", filename=\"model.safetensors\")\n",
    "tokenizer_path = hf_hub_download(repo_id=\"tsingla98/frawdllm-100m\", filename=\"tokenizer.json\")\n",
    "\n",
    "print(f\"Weights: {weights_path}\")\n",
    "print(f\"Tokenizer: {tokenizer_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Load weights into JAX arrays\n",
    "weights = {}\n",
    "with safe_open(weights_path, framework=\"numpy\") as f:\n",
    "    for key in f.keys():\n",
    "        weights[key] = jnp.array(f.get_tensor(key))\n",
    "        print(f\"{key}: {weights[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import math\n",
    "\n",
    "EMBEDDINGS_WEIGHT_KEY = \"model.embeddings.token_emb.weight\"\n",
    "\n",
    "LN1_WEIGHT_FORMAT_PRE_ATTN_KEY = \"model.blocks.{}.ln1.weight\"\n",
    "LN1_BIAS_FORMAT_PRE_ATTN_KEY = \"model.blocks.{}.ln1.bias\"\n",
    "\n",
    "LN2_WEIGHT_FORMAT_PRE_ATTN_KEY = \"model.blocks.{}.ln2.weight\"\n",
    "LN2_BIAS_FORMAT_PRE_ATTN_KEY = \"model.blocks.{}.ln2.bias\"\n",
    "\n",
    "LNF_WEIGHT_FORMAT_KEY = \"model.ln_f.weight\"\n",
    "LNF_BIAS_FORMAT_KEY = \"model.ln_f.bias\"\n",
    "\n",
    "LM_HEAD_WEIGHT_KEY = \"model.embeddings.token_emb.weight\"  # tied weights\n",
    "\n",
    "QKV_WEIGHTS_KEY = \"model.blocks.{}.attn.qkv_proj.weight\"\n",
    "QKV_BIAS_KEY = \"model.blocks.{}.attn.qkv_proj.bias\"\n",
    "\n",
    "OUTPUT_PROJ_BIAS_KEY = \"model.blocks.{}.attn.out_proj.bias\"\n",
    "OUTPUT_PROJ_WEIGHT_KEY = \"model.blocks.{}.attn.out_proj.weight\"\n",
    "\n",
    "FC1_WEIGHT_KEY = \"model.blocks.{}.mlp.fc1.weight\"\n",
    "FC1_BIAS_KEY = \"model.blocks.{}.mlp.fc1.bias\"\n",
    "\n",
    "FC2_WEIGHT_KEY = \"model.blocks.{}.mlp.fc2.weight\"\n",
    "FC2_BIAS_KEY = \"model.blocks.{}.mlp.fc2.bias\"\n",
    "\n",
    "STOP_TOKEN_ID = 3\n",
    "\n",
    "EPSILON = 1e-5\n",
    "N_HEADS = 12\n",
    "N_LAYERS = 12\n",
    "HEAD_DIM = 64\n",
    "EMBEDDINGS_DIM = N_HEADS * HEAD_DIM\n",
    "ROPE_THETA = 10000.0\n",
    "TEMPERATURE = 0.5\n",
    "TOP_P = 0.9\n",
    "\n",
    "MAX_OUTPUT_TOKENS = 300\n",
    "MAX_CONTEXT_LENGTH = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Implementation (JAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def format_prompt(user_message: str) -> str:\n",
    "    return f\"<|bos|><|user|>{user_message}<|assistant|>\"\n",
    "\n",
    "\n",
    "def get_weights_tensor(key: str) -> jnp.ndarray:\n",
    "    return weights[key]\n",
    "\n",
    "\n",
    "def get_tokens_for_prompt(prompt: str) -> jnp.ndarray:\n",
    "    return jnp.array(tokenizer.encode(prompt, add_special_tokens=False).ids)\n",
    "\n",
    "\n",
    "def get_embeddings_for_selection(selection: jnp.ndarray) -> jnp.ndarray:\n",
    "    return get_weights_tensor(EMBEDDINGS_WEIGHT_KEY)[selection]\n",
    "\n",
    "\n",
    "# Precompute RoPE frequencies for max context length\n",
    "def _compute_rope_freqs(max_len: int) -> jnp.ndarray:\n",
    "    dim_indices = jnp.arange(0, HEAD_DIM, 2).astype(jnp.float32)\n",
    "    freqs = 1.0 / (ROPE_THETA ** (dim_indices / HEAD_DIM))\n",
    "    positions = jnp.arange(max_len).astype(jnp.float32)\n",
    "    angles = jnp.outer(positions, freqs)\n",
    "    return angles\n",
    "\n",
    "ROPE_FREQS = _compute_rope_freqs(MAX_CONTEXT_LENGTH)\n",
    "\n",
    "\n",
    "def apply_rope(x: jnp.ndarray, angles: jnp.ndarray) -> jnp.ndarray:\n",
    "    x_even = x[..., 0::2]\n",
    "    x_odd = x[..., 1::2]\n",
    "    cos = jnp.cos(angles)\n",
    "    sin = jnp.sin(angles)\n",
    "    x_even_rot = x_even * cos - x_odd * sin\n",
    "    x_odd_rot = x_even * sin + x_odd * cos\n",
    "    out = jnp.stack([x_even_rot, x_odd_rot], axis=-1)\n",
    "    out = out.reshape(x.shape)\n",
    "    return out\n",
    "\n",
    "\n",
    "# === PREFILL: Process full prompt (first pass) ===\n",
    "@functools.partial(jax.jit, static_argnums=(0,))\n",
    "def prefill_attention(\n",
    "    layer_num: int, x: jnp.ndarray, k_cache: jnp.ndarray, v_cache: jnp.ndarray\n",
    ") -> tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, int]:\n",
    "    seq_len = x.shape[0]\n",
    "\n",
    "    gamma = get_weights_tensor(LN1_WEIGHT_FORMAT_PRE_ATTN_KEY.format(layer_num))\n",
    "    beta = get_weights_tensor(LN1_BIAS_FORMAT_PRE_ATTN_KEY.format(layer_num))\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    std = x.std(axis=-1, keepdims=True)\n",
    "    output = (x - mean) / (std + EPSILON)\n",
    "    output = output * gamma + beta\n",
    "\n",
    "    w_qkv = get_weights_tensor(QKV_WEIGHTS_KEY.format(layer_num))\n",
    "    qkv = output @ w_qkv.T\n",
    "    qkv_bias = get_weights_tensor(QKV_BIAS_KEY.format(layer_num))\n",
    "    qkv = qkv + qkv_bias\n",
    "\n",
    "    q, k, v = jnp.split(qkv, 3, axis=-1)\n",
    "    q_batched = q.reshape(seq_len, N_HEADS, HEAD_DIM).swapaxes(0, 1)\n",
    "    k_batched = k.reshape(seq_len, N_HEADS, HEAD_DIM).swapaxes(0, 1)\n",
    "    v_batched = v.reshape(seq_len, N_HEADS, HEAD_DIM).swapaxes(0, 1)\n",
    "\n",
    "    # Use precomputed RoPE, slice to seq_len\n",
    "    angles = ROPE_FREQS[:seq_len]\n",
    "    q_batched = apply_rope(q_batched, angles)\n",
    "    k_batched = apply_rope(k_batched, angles)\n",
    "\n",
    "    # Store in cache\n",
    "    k_cache = k_cache.at[:seq_len, :, :].set(k_batched.swapaxes(0, 1))\n",
    "    v_cache = v_cache.at[:seq_len, :, :].set(v_batched.swapaxes(0, 1))\n",
    "\n",
    "    # Attention with causal mask\n",
    "    res_batched = q_batched @ k_batched.swapaxes(-2, -1)\n",
    "    mask = jnp.triu(jnp.ones([seq_len, seq_len]), k=1) * -1e9\n",
    "    res_batched = res_batched + mask\n",
    "    res_batched = res_batched / math.sqrt(HEAD_DIM)\n",
    "    res_batched = jax.nn.softmax(res_batched, axis=-1)\n",
    "\n",
    "    out = res_batched @ v_batched\n",
    "    out = out.swapaxes(0, 1).reshape(seq_len, EMBEDDINGS_DIM)\n",
    "\n",
    "    output_proj_weights = get_weights_tensor(OUTPUT_PROJ_WEIGHT_KEY.format(layer_num))\n",
    "    output_proj_bias = get_weights_tensor(OUTPUT_PROJ_BIAS_KEY.format(layer_num))\n",
    "    out = out @ output_proj_weights.T + output_proj_bias + x\n",
    "\n",
    "    return out, k_cache, v_cache, seq_len\n",
    "\n",
    "\n",
    "# === DECODE: Process single token (subsequent passes) ===\n",
    "# Note: cache_len is NOT static - we use masking instead of dynamic slicing\n",
    "@functools.partial(jax.jit, static_argnums=(0,))\n",
    "def decode_attention(\n",
    "    layer_num: int, x: jnp.ndarray, k_cache: jnp.ndarray, v_cache: jnp.ndarray, cache_len: jnp.ndarray\n",
    ") -> tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n",
    "    # x is [1, 768] - single token\n",
    "    # cache_len is a scalar JAX array (traced)\n",
    "    new_seq_len = cache_len + 1\n",
    "\n",
    "    gamma = get_weights_tensor(LN1_WEIGHT_FORMAT_PRE_ATTN_KEY.format(layer_num))\n",
    "    beta = get_weights_tensor(LN1_BIAS_FORMAT_PRE_ATTN_KEY.format(layer_num))\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    std = x.std(axis=-1, keepdims=True)\n",
    "    output = (x - mean) / (std + EPSILON)\n",
    "    output = output * gamma + beta\n",
    "\n",
    "    w_qkv = get_weights_tensor(QKV_WEIGHTS_KEY.format(layer_num))\n",
    "    qkv = output @ w_qkv.T\n",
    "    qkv_bias = get_weights_tensor(QKV_BIAS_KEY.format(layer_num))\n",
    "    qkv = qkv + qkv_bias\n",
    "\n",
    "    q, k, v = jnp.split(qkv, 3, axis=-1)\n",
    "    q_batched = q.reshape(1, N_HEADS, HEAD_DIM).swapaxes(0, 1)  # [N_HEADS, 1, HEAD_DIM]\n",
    "    k_batched = k.reshape(1, N_HEADS, HEAD_DIM).swapaxes(0, 1)\n",
    "    v_batched = v.reshape(1, N_HEADS, HEAD_DIM).swapaxes(0, 1)\n",
    "\n",
    "    # RoPE for position cache_len (the new token's position) - use precomputed, slice dynamically\n",
    "    angles = jax.lax.dynamic_slice(ROPE_FREQS, (cache_len, 0), (1, HEAD_DIM // 2))\n",
    "    q_batched = apply_rope(q_batched, angles)\n",
    "    k_batched = apply_rope(k_batched, angles)\n",
    "\n",
    "    # Append to cache\n",
    "    k_cache = k_cache.at[cache_len, :, :].set(k_batched.swapaxes(0, 1)[0])\n",
    "    v_cache = v_cache.at[cache_len, :, :].set(v_batched.swapaxes(0, 1)[0])\n",
    "\n",
    "    # Use FULL cache but mask out positions beyond new_seq_len\n",
    "    # k_cache: [MAX_CONTEXT_LENGTH, N_HEADS, HEAD_DIM]\n",
    "    k_full = k_cache.swapaxes(0, 1)  # [N_HEADS, MAX_CONTEXT_LENGTH, HEAD_DIM]\n",
    "    v_full = v_cache.swapaxes(0, 1)  # [N_HEADS, MAX_CONTEXT_LENGTH, HEAD_DIM]\n",
    "\n",
    "    # Attention with masking\n",
    "    # q_batched: [N_HEADS, 1, HEAD_DIM]\n",
    "    # k_full: [N_HEADS, MAX_CONTEXT_LENGTH, HEAD_DIM]\n",
    "    res_batched = q_batched @ k_full.swapaxes(-2, -1)  # [N_HEADS, 1, MAX_CONTEXT_LENGTH]\n",
    "    res_batched = res_batched / math.sqrt(HEAD_DIM)\n",
    "    \n",
    "    # Mask out positions >= new_seq_len (they're garbage/zero)\n",
    "    positions = jnp.arange(MAX_CONTEXT_LENGTH)\n",
    "    mask = jnp.where(positions < new_seq_len, 0.0, -1e9)  # [MAX_CONTEXT_LENGTH]\n",
    "    res_batched = res_batched + mask  # broadcast to [N_HEADS, 1, MAX_CONTEXT_LENGTH]\n",
    "    \n",
    "    res_batched = jax.nn.softmax(res_batched, axis=-1)\n",
    "\n",
    "    out = res_batched @ v_full  # [N_HEADS, 1, HEAD_DIM]\n",
    "    out = out.swapaxes(0, 1).reshape(1, EMBEDDINGS_DIM)\n",
    "\n",
    "    output_proj_weights = get_weights_tensor(OUTPUT_PROJ_WEIGHT_KEY.format(layer_num))\n",
    "    output_proj_bias = get_weights_tensor(OUTPUT_PROJ_BIAS_KEY.format(layer_num))\n",
    "    out = out @ output_proj_weights.T + output_proj_bias + x\n",
    "\n",
    "    return out, k_cache, v_cache, new_seq_len\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnums=(0,))\n",
    "def process_mlp(layer_num: int, x: jnp.ndarray) -> jnp.ndarray:\n",
    "    gamma = get_weights_tensor(LN2_WEIGHT_FORMAT_PRE_ATTN_KEY.format(layer_num))\n",
    "    beta = get_weights_tensor(LN2_BIAS_FORMAT_PRE_ATTN_KEY.format(layer_num))\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    std = x.std(axis=-1, keepdims=True)\n",
    "    output = (x - mean) / (std + EPSILON)\n",
    "    output = output * gamma + beta\n",
    "    fc1_weights = get_weights_tensor(FC1_WEIGHT_KEY.format(layer_num))\n",
    "    fc1_bias = get_weights_tensor(FC1_BIAS_KEY.format(layer_num))\n",
    "    fc2_weights = get_weights_tensor(FC2_WEIGHT_KEY.format(layer_num))\n",
    "    fc2_bias = get_weights_tensor(FC2_BIAS_KEY.format(layer_num))\n",
    "    output = output @ fc1_weights.T + fc1_bias\n",
    "    output = jax.nn.gelu(output)\n",
    "    output = output @ fc2_weights.T + fc2_bias\n",
    "    return output + x\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def process_final_layer(x: jnp.ndarray) -> jnp.ndarray:\n",
    "    gamma = get_weights_tensor(LNF_WEIGHT_FORMAT_KEY)\n",
    "    beta = get_weights_tensor(LNF_BIAS_FORMAT_KEY)\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    std = x.std(axis=-1, keepdims=True)\n",
    "    output = (x - mean) / (std + EPSILON)\n",
    "    output = output * gamma + beta\n",
    "    lm_head = get_weights_tensor(LM_HEAD_WEIGHT_KEY)\n",
    "    return output @ lm_head.T\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    prompt = \"Hello, World!\"\n",
    "    prompt = format_prompt(prompt)\n",
    "    initial_tokens = get_tokens_for_prompt(prompt)\n",
    "\n",
    "    all_tokens = jnp.zeros(len(initial_tokens) + MAX_OUTPUT_TOKENS, dtype=jnp.int32)\n",
    "    all_tokens = all_tokens.at[: len(initial_tokens)].set(initial_tokens)\n",
    "    num_tokens = len(initial_tokens)\n",
    "    prompt_len = len(initial_tokens)\n",
    "\n",
    "    k_caches = [jnp.zeros((MAX_CONTEXT_LENGTH, N_HEADS, HEAD_DIM)) for _ in range(N_LAYERS)]\n",
    "    v_caches = [jnp.zeros((MAX_CONTEXT_LENGTH, N_HEADS, HEAD_DIM)) for _ in range(N_LAYERS)]\n",
    "    # Use JAX arrays for cache_lens so they can be traced\n",
    "    cache_lens = [jnp.array(0, dtype=jnp.int32) for _ in range(N_LAYERS)]\n",
    "\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    is_prefill = True\n",
    "\n",
    "    for _ in range(min(MAX_OUTPUT_TOKENS, MAX_CONTEXT_LENGTH - prompt_len)):\n",
    "        tokens = all_tokens[:num_tokens]\n",
    "        \n",
    "        if is_prefill:\n",
    "            # First pass: process full prompt\n",
    "            x = get_embeddings_for_selection(tokens)\n",
    "            for layer_num in range(N_LAYERS):\n",
    "                x, k_caches[layer_num], v_caches[layer_num], cache_len_out = prefill_attention(\n",
    "                    layer_num, x, k_caches[layer_num], v_caches[layer_num]\n",
    "                )\n",
    "                cache_lens[layer_num] = jnp.array(cache_len_out, dtype=jnp.int32)\n",
    "                x = process_mlp(layer_num, x)\n",
    "            is_prefill = False\n",
    "        else:\n",
    "            # Subsequent passes: process only last token\n",
    "            x = get_embeddings_for_selection(tokens[-1:])\n",
    "            for layer_num in range(N_LAYERS):\n",
    "                x, k_caches[layer_num], v_caches[layer_num], cache_lens[layer_num] = decode_attention(\n",
    "                    layer_num, x, k_caches[layer_num], v_caches[layer_num], cache_lens[layer_num]\n",
    "                )\n",
    "                x = process_mlp(layer_num, x)\n",
    "\n",
    "        x = process_final_layer(x)\n",
    "\n",
    "        logits = x[-1] / TEMPERATURE\n",
    "        probs = jax.nn.softmax(logits, axis=-1)\n",
    "\n",
    "        top_probs, top_indices = jax.lax.top_k(probs, k=100)\n",
    "        sorted_indices = jnp.argsort(top_probs)[::-1]\n",
    "        sorted_probs = top_probs[sorted_indices]\n",
    "        cum_probs = jnp.cumsum(sorted_probs, axis=0)\n",
    "        cum_probs_keep = cum_probs <= TOP_P\n",
    "        cum_probs_keep = cum_probs_keep.at[0].set(True)\n",
    "        sorted_probs = jnp.where(cum_probs_keep, sorted_probs, 0.0)\n",
    "\n",
    "        key, subkey = jax.random.split(key)\n",
    "        sampled_idx = jax.random.categorical(subkey, jnp.log(sorted_probs + 1e-10))\n",
    "        next_token = top_indices[sorted_indices[sampled_idx]]\n",
    "\n",
    "        if next_token == STOP_TOKEN_ID:\n",
    "            break\n",
    "        all_tokens = all_tokens.at[num_tokens].set(next_token)\n",
    "        num_tokens += 1\n",
    "\n",
    "    output_tokens = all_tokens[prompt_len:num_tokens].tolist()\n",
    "    output_words = tokenizer.decode(output_tokens)\n",
    "    print(output_words)\n",
    "\n",
    "\n",
    "# Run it!\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frawdllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
